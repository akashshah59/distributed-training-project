{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are GPU's available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘tiny-imagenet-200/*.txt’: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm tiny-imagenet-200/*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘tiny-imagenet-200/val/*.txt’: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm tiny-imagenet-200/val/*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='tiny-imagenet-200/train',\n",
    "                                           transform=data_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = 128,\n",
    "                                             shuffle=True\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = datasets.ImageFolder(root='tiny-imagenet-200/val',\n",
    "                                           transform=data_transform)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             shuffle=False\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.ImageFolder(root='tiny-imagenet-200/test',\n",
    "                                           transform=data_transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                             shuffle=False\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (resnet_50): DataParallel(\n",
       "    (module): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (output): Linear(in_features=1000, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Uncomment for parallelism\n",
    "        self.resnet_50 = nn.DataParallel(models.resnet50(pretrained = True),device_ids = [0,1,2])\n",
    "        #self.resnet_50 = models.resnet50(pretrained = True)\n",
    "        self.output = nn.Linear(1000, 200)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet_50(x)\n",
    "        x = F.relu(x)\n",
    "        # No need for softmax as CrossEntropy already implements it\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, minibatch number    10 training loss: 6.045 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number    20 training loss: 5.716 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number    30 training loss: 5.486 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number    40 training loss: 5.207 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number    50 training loss: 4.999 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number    60 training loss: 4.767 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number    70 training loss: 4.578 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number    80 training loss: 4.330 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number    90 training loss: 4.156 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   100 training loss: 3.917 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   110 training loss: 3.756 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   120 training loss: 3.684 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   130 training loss: 3.424 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   140 training loss: 3.225 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   150 training loss: 3.104 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   160 training loss: 2.956 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   170 training loss: 2.862 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   180 training loss: 2.975 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   190 training loss: 2.823 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   200 training loss: 2.822 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   210 training loss: 2.713 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   220 training loss: 2.579 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   230 training loss: 2.500 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   240 training loss: 2.532 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   250 training loss: 2.485 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   260 training loss: 2.502 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   270 training loss: 2.491 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   280 training loss: 2.476 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   290 training loss: 2.341 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   300 training loss: 2.349 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   310 training loss: 2.348 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   320 training loss: 2.317 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   330 training loss: 2.267 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   340 training loss: 2.316 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   350 training loss: 2.252 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   360 training loss: 2.240 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   370 training loss: 2.358 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   380 training loss: 2.201 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   390 training loss: 2.117 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   400 training loss: 2.276 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   410 training loss: 2.018 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   420 training loss: 2.116 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   430 training loss: 2.145 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   440 training loss: 1.992 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   450 training loss: 2.159 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   460 training loss: 2.168 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   470 training loss: 2.035 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   480 training loss: 2.054 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   490 training loss: 2.122 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   500 training loss: 2.121 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   510 training loss: 2.061 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   520 training loss: 2.062 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   530 training loss: 2.036 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   540 training loss: 2.045 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   550 training loss: 2.007 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   560 training loss: 1.942 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   570 training loss: 2.000 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   580 training loss: 2.013 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   590 training loss: 2.026 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   600 training loss: 1.978 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   610 training loss: 1.998 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   620 training loss: 1.941 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   630 training loss: 2.092 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   640 training loss: 1.943 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   650 training loss: 1.999 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   660 training loss: 1.976 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   670 training loss: 1.974 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   680 training loss: 2.019 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   690 training loss: 1.936 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   700 training loss: 1.882 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   710 training loss: 1.839 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   720 training loss: 1.796 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   730 training loss: 1.783 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   740 training loss: 1.905 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   750 training loss: 1.969 and time taken is 0.104 seconds\n",
      "epoch 1, minibatch number   760 training loss: 1.873 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   770 training loss: 1.815 and time taken is 0.103 seconds\n",
      "epoch 1, minibatch number   780 training loss: 1.931 and time taken is 0.104 seconds\n",
      "Epoch 1 took 4.847 minutes\n",
      "\n",
      "epoch 2, minibatch number    10 training loss: 1.541 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number    20 training loss: 1.560 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number    30 training loss: 1.495 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number    40 training loss: 1.456 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number    50 training loss: 1.498 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number    60 training loss: 1.550 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number    70 training loss: 1.386 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number    80 training loss: 1.594 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number    90 training loss: 1.434 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   100 training loss: 1.542 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   110 training loss: 1.567 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   120 training loss: 1.453 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   130 training loss: 1.565 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   140 training loss: 1.409 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   150 training loss: 1.543 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   160 training loss: 1.413 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   170 training loss: 1.480 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   180 training loss: 1.447 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   190 training loss: 1.445 and time taken is 0.104 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, minibatch number   200 training loss: 1.483 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   210 training loss: 1.538 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   220 training loss: 1.516 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   230 training loss: 1.539 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   240 training loss: 1.456 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   250 training loss: 1.532 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   260 training loss: 1.404 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   270 training loss: 1.519 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   280 training loss: 1.551 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   290 training loss: 1.456 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   300 training loss: 1.454 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   310 training loss: 1.423 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   320 training loss: 1.489 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   330 training loss: 1.476 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   340 training loss: 1.412 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   350 training loss: 1.440 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   360 training loss: 1.502 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   370 training loss: 1.487 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   380 training loss: 1.428 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   390 training loss: 1.442 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   400 training loss: 1.434 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   410 training loss: 1.421 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   420 training loss: 1.414 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   430 training loss: 1.489 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   440 training loss: 1.502 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   450 training loss: 1.512 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   460 training loss: 1.471 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   470 training loss: 1.406 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   480 training loss: 1.441 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   490 training loss: 1.382 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   500 training loss: 1.505 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   510 training loss: 1.487 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   520 training loss: 1.406 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   530 training loss: 1.446 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   540 training loss: 1.466 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   550 training loss: 1.477 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   560 training loss: 1.398 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   570 training loss: 1.448 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   580 training loss: 1.446 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   590 training loss: 1.367 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   600 training loss: 1.422 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   610 training loss: 1.496 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   620 training loss: 1.484 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   630 training loss: 1.430 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   640 training loss: 1.469 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   650 training loss: 1.453 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   660 training loss: 1.426 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   670 training loss: 1.480 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   680 training loss: 1.481 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   690 training loss: 1.490 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   700 training loss: 1.449 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   710 training loss: 1.452 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   720 training loss: 1.523 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   730 training loss: 1.454 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   740 training loss: 1.455 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   750 training loss: 1.496 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   760 training loss: 1.474 and time taken is 0.104 seconds\n",
      "epoch 2, minibatch number   770 training loss: 1.417 and time taken is 0.103 seconds\n",
      "epoch 2, minibatch number   780 training loss: 1.448 and time taken is 0.104 seconds\n",
      "Epoch 2 took 4.720 minutes\n",
      "\n",
      "\n",
      "Finished Training after 0.159 hours\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start = time()\n",
    "\n",
    "for epoch in range(2):\n",
    "    start_epoch = time()\n",
    "    running_loss=0.0\n",
    "    for i, data in enumerate(train_loader, start = 0):\n",
    "        inputs, labels = data\n",
    "        labels = labels.to('cuda:0') #Move to GPU\n",
    "        start_mini = time()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        end_mini = time()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 1000 mini-batches\n",
    "            print('epoch %d, minibatch number %5d training loss: %.3f and time taken is %.3f seconds' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10,(end_mini - start_mini)))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    end_epoch = time()\n",
    "    print(\"Epoch %d took %.3f minutes\"%(epoch + 1,(end_epoch - start_epoch )/60))\n",
    "    print(\"\")\n",
    "\n",
    "end = time()    \n",
    "print(\"\")\n",
    "print('Finished Training after %.3f hours'%((end-start) / 60 / 60 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_accuracy(loader):\n",
    "    with torch.no_grad():\n",
    "            net.to('cuda:0')\n",
    "            net.eval()\n",
    "            count = 0\n",
    "            for inputs in loader:\n",
    "                inputs ,labels = inputs\n",
    "                inputs = inputs.to('cuda:0')\n",
    "                labels = labels.to('cuda:0')\n",
    "                predictions = net(inputs)\n",
    "                _, predicted = torch.max(predictions, 1)\n",
    "                if predicted == labels:\n",
    "                    count = count + 1\n",
    "            return count / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:\n",
      "0.005\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy:\")\n",
    "print(fetch_accuracy(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
